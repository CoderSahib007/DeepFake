{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing model's base architecture\n",
    "from transformers import Wav2Vec2Model\n",
    "\n",
    "# Load the model\n",
    "model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "# Print architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, layer in model.named_children():\n",
    "    print(name, \"->\", layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model, input_size=(1, 1, 16000))  # Assuming 1 second of audio at 16kHz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import Wav2Vec2PreTrainedModel, Wav2Vec2Model, Wav2Vec2Processor\n",
    "import torchaudio\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wav2Vec2ForAudioClassification(Wav2Vec2PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        \n",
    "        # Base wav2vec2 model\n",
    "        self.wav2vec2 = Wav2Vec2Model(config)\n",
    "        self.dropout = nn.Dropout(config.final_dropout)\n",
    "        \n",
    "        # Classification head: Linear layer for binary output (0 or 1)\n",
    "        hidden_size = config.hidden_size\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 128),  # Reduce dimensionality\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(128, 1)  # Single output for binary classification\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.post_init()\n",
    "    \n",
    "    def freeze_feature_encoder(self):\n",
    "        \"\"\"Freeze the feature encoder to prevent updates during training.\"\"\"\n",
    "        self.wav2vec2.feature_extractor._freeze_parameters()\n",
    "    \n",
    "    def freeze_base_model(self):\n",
    "        \"\"\"Freeze the base model, only train the classifier.\"\"\"\n",
    "        for param in self.wav2vec2.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_values: torch.Tensor,\n",
    "        attention_mask: torch.Tensor = None,\n",
    "        labels: torch.Tensor = None,  # Binary labels (0 or 1)\n",
    "        output_attentions: bool = None,\n",
    "        output_hidden_states: bool = None,\n",
    "        return_dict: bool = None,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        \n",
    "        # Extract features from wav2vec2\n",
    "        outputs = self.wav2vec2(\n",
    "            input_values,\n",
    "            attention_mask=attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        \n",
    "        # Use the last hidden state (CLS-like pooling: mean over time)\n",
    "        hidden_states = outputs[0]  # Shape: (batch_size, sequence_length, hidden_size)\n",
    "        pooled_output = hidden_states.mean(dim=1)  # Mean pooling: (batch_size, hidden_size)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        # Classification logits\n",
    "        logits = self.classifier(pooled_output)  # Shape: (batch_size, 1)\n",
    "        \n",
    "        # Loss computation\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Ensure labels are float for BCEWithLogitsLoss\n",
    "            labels = labels.view(-1, 1).float()\n",
    "            loss_fn = nn.BCEWithLogitsLoss()\n",
    "            loss = loss_fn(logits, labels)\n",
    "        \n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "        \n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio(audio_paths, processor, model=None, sampling_rate=16000, perturbation_steps=3, epsilon=0.01):\n",
    "    \"\"\"Preprocess audio files with F-SAT perturbations for robustness training.\"\"\"\n",
    "    input_values = []\n",
    "    for audio_path in audio_paths:\n",
    "        # Load and resample audio\n",
    "        waveform, sr = torchaudio.load(audio_path)\n",
    "        if sr != sampling_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr, sampling_rate)\n",
    "            waveform = resampler(waveform)\n",
    "        if waveform.shape[0] > 1:  # Convert to mono\n",
    "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "        \n",
    "        # Convert to tensor and ensure 1D\n",
    "        audio = waveform.squeeze().to(device).float()\n",
    "        \n",
    "        # Apply F-SAT perturbations if model is provided (for training)\n",
    "        if model is not None and model.training:\n",
    "            audio = audio.unsqueeze(0)  # Add batch dimension\n",
    "            \n",
    "            # STFT to get magnitude and phase\n",
    "            window_size = 512\n",
    "            hop_length = 128\n",
    "            stft = torch.stft(audio, n_fft=window_size, hop_length=hop_length, return_complex=True)\n",
    "            magnitude, phase = torch.abs(stft), torch.angle(stft)\n",
    "            \n",
    "            # Initialize perturbation\n",
    "            perturbation = torch.zeros_like(magnitude, device=device, requires_grad=True)\n",
    "            \n",
    "            # Iterative perturbation (simplified F-SAT)\n",
    "            for _ in range(perturbation_steps):\n",
    "                # Reconstruct perturbed audio\n",
    "                perturbed_magnitude = magnitude + perturbation\n",
    "                perturbed_stft = perturbed_magnitude * torch.exp(1j * phase)\n",
    "                perturbed_audio = torch.istft(perturbed_stft, n_fft=window_size, hop_length=hop_length, length=audio.shape[-1])\n",
    "                \n",
    "                # Process through model to get loss\n",
    "                processed = processor(perturbed_audio.squeeze().cpu().numpy(), \n",
    "                                    sampling_rate=sampling_rate, \n",
    "                                    return_tensors=\"pt\", padding=True)\n",
    "                processed = {k: v.to(device) for k, v in processed.items()}\n",
    "                \n",
    "                outputs = model(processed[\"input_values\"], attention_mask=processed[\"attention_mask\"])\n",
    "                loss = outputs.loss\n",
    "                loss.backward()\n",
    "                \n",
    "                # Update perturbation (gradient ascent to maximize loss)\n",
    "                with torch.no_grad():\n",
    "                    perturbation += epsilon * perturbation.grad / (perturbation.grad.norm() + 1e-8)\n",
    "                    perturbation.clamp_(-epsilon, epsilon)  # Constrain perturbation magnitude\n",
    "                    perturbation.grad.zero_()\n",
    "            \n",
    "            # Apply final perturbation and reconstruct\n",
    "            perturbed_magnitude = magnitude + perturbation\n",
    "            perturbed_stft = perturbed_magnitude * torch.exp(1j * phase)\n",
    "            perturbed_audio = torch.istft(perturbed_stft, n_fft=window_size, hop_length=hop_length, length=audio.shape[-1])\n",
    "            audio = perturbed_audio.squeeze()\n",
    "        \n",
    "        # Process with wav2vec2 processor\n",
    "        audio_array = audio.cpu().numpy()\n",
    "        inputs = processor(audio_array, sampling_rate=sampling_rate, return_tensors=\"pt\", padding=True)\n",
    "        input_values.append(inputs[\"input_values\"].squeeze(0))\n",
    "    \n",
    "    # Pad sequences to the same length\n",
    "    input_values = torch.nn.utils.rnn.pad_sequence(input_values, batch_first=True)\n",
    "    attention_mask = (input_values != 0).long()  # Mask for padded regions\n",
    "    return {\"input_values\": input_values, \"attention_mask\": attention_mask}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data (replace with your actual paths and labels)\n",
    "data = {\n",
    "    \"audio\": [\"path/to/audio1.wav\", \"path/to/audio2.wav\", \"path/to/audio3.wav\"],\n",
    "    \"labels\": [0, 1, 0]  # Binary labels\n",
    "}\n",
    "dataset = Dataset.from_dict(data)\n",
    "\n",
    "# Load processor and model\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2ForAudioClassification.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model.to(device)\n",
    "model.freeze_base_model()  # Optional: Freeze base model\n",
    "\n",
    "# Preprocess dataset with perturbations\n",
    "def preprocess_batch(examples):\n",
    "    processed = preprocess_audio(examples[\"audio\"], processor, model=model)\n",
    "    processed[\"labels\"] = examples[\"labels\"]\n",
    "    return processed\n",
    "\n",
    "processed_dataset = dataset.map(preprocess_batch, batched=True, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./wav2vec2_classification\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    ")\n",
    "\n",
    "# Compute metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = (torch.sigmoid(torch.tensor(logits)) > 0.5).int()\n",
    "    accuracy = (predictions == torch.tensor(labels)).float().mean().item()\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=processed_dataset,\n",
    "    eval_dataset=processed_dataset,  # Use a separate validation set in practice\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example inference\n",
    "test_audio = [\"path/to/test_audio.wav\"]\n",
    "processed_test = preprocess_audio(test_audio, processor, model=None)  # No perturbation during inference\n",
    "input_values = processed_test[\"input_values\"].to(device)\n",
    "attention_mask = processed_test[\"attention_mask\"].to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_values, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "    prediction = (torch.sigmoid(logits) > 0.5).int().item()\n",
    "    print(f\"Predicted class: {prediction}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
